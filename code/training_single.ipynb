{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Superconductivity Regression Notebook - Testing Single Models**\n",
    "Trains models to predict critical temperatures based on features found with \"*../code/get_featurizers.ipynb*\". Imports data from \"*../data/supercon_feat.csv*\", which is produced in *get_featurizers.ipynb*. The orginal data is from the supercon database. This notebook is for testing single models.\n",
    "\n",
    "*Author: Kirk Kleinsasser*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "#dill.load_session('../data/latest-run.db') #this can load a saved python session so I don't need to rerun computationally expensive cells\n",
    "%autosave 300 \n",
    "#autosaves code every five minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries / Define Import Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "#general imports:\n",
    "import warnings #to suppress grid search warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #heatmaps\n",
    "\n",
    "#regression models:\n",
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#various ML tools:\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, r2_score, mean_absolute_error, mean_squared_error\n",
    "from skopt import BayesSearchCV #bayesian optimization\n",
    "\n",
    "#imports the data from get_featurizers. Function because some models we may want infinity:\n",
    "def import_data(replace_inf=False):\n",
    "    global data, target, train_data, test_data, train_target, test_target #variables that we want to define globally (outside of this funtion)\n",
    "    data = pd.DataFrame(pd.read_csv('../data/supercon_feat.csv')) #loads data produced in get_featurizer.ipynb\n",
    "    target = data.pop('Tc') #remove target (critical temp) from data\n",
    "\n",
    "    if replace_inf: #replaces values of infinity with NaN if replace_inf is True\n",
    "        data.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "\n",
    "    #TODO: debug feaurizers - NaN is entered when there is an error in the featurizer\n",
    "    data.drop(['name','Unnamed: 0', 'composition'], axis=1, inplace=True) #drop columns irrelevant to training\n",
    "    data = data[data.columns[data.notnull().any()]] #drop columns that are entirely NaN (12 columns) \n",
    "\n",
    "    for col in data: #replaces NaN with zeros\n",
    "        data[col] = pd.to_numeric(data[col], errors ='coerce').fillna(0).astype('float')\n",
    "\n",
    "    #creates a test train split, with shuffle and random state for reproducibility \n",
    "    train_data, test_data, train_target, test_target = train_test_split(data, target, test_size=0.15, random_state=43, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "To train models and return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one(model_name, regressor, parameters): #define function that trains a model and prints scores and plots\n",
    "    global train_data, train_data, test_data, test_target #we need these variables and don't want to pass them as arguments\n",
    "    with plt.rc_context({'xtick.color':'white', 'ytick.color':'white','axes.titlecolor':'white','figure.facecolor':'#1e1e1e','text.color':'white','legend.labelcolor':'black'}):\n",
    "        plt.title(f\"{model_name} - Prediction vs. Actual Value (CV)\", color='white')\n",
    "        model = regressor(**parameters) #unpacks model and params\n",
    "        model.fit(train_data.values, train_target.values) #fit the model\n",
    "        model_pred = model.predict(test_data) #make predictions on test data\n",
    "\n",
    "        mse = round(mean_squared_error(test_target, model_pred),3) #find mean square error\n",
    "        r_squared = round(r2_score(test_target, model_pred),3) #find r2 score\n",
    "\n",
    "        #make our plot - with plt.rc_context sets theme to look good in dark mode\n",
    "        difference = np.abs(test_target - model_pred) #function that finds the absolute difference between predicted and actual value\n",
    "        im = plt.scatter(model_pred, test_target, cmap='plasma_r', norm=plt.Normalize(0, 120), c=difference, label=\"Critical Temperature (K)\") #create scatter plot of data \n",
    "        plt.plot((0,135), (0,135), 'k--', alpha=0.75) #add expected line. Values must be changed with different data to look good\n",
    "        plt.title(model_name, c='white')\n",
    "        plt.ylabel('Prediction', c='white')\n",
    "        plt.xlabel('Actual Value', c='white')\n",
    "        plt.annotate(f'MSE: {mse}', xy = (1.0, -0.15), xycoords='axes fraction', ha='right', va=\"center\", fontsize=10) #add footnote with MSE\n",
    "        plt.annotate(f'R2: {r_squared}', xy = (0.18, -0.15), xycoords='axes fraction', ha='right', va=\"center\", fontsize=10) #add footnote with R2 \n",
    "\n",
    "        plt.legend()\n",
    "        plt.colorbar().set_label(label=\"Difference from Actual (K)\", color='white') #using .set_label() as colorbar() does accept color arguments\n",
    "        plt.show()\n",
    "# evaluate_one(\"Elastic Net Regression\", ElasticNet, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 442.16314939573556\n",
      "R2 : 0.3918631341771974\n"
     ]
    }
   ],
   "source": [
    "import_data(replace_inf=True) #reimport data without infinities\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(train_data, train_target)\n",
    "linear_pred = linear.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, linear_pred)\n",
    "r_squared = r2_score(test_target, linear_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 267.72718623518745\n",
      "R2 : 0.6317767046957012\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(kernel='rbf', C=100, epsilon=0.1, gamma=0.1, degree=1)\n",
    "svr.fit(train_data, train_target)\n",
    "svr_pred = svr.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, svr_pred)\n",
    "r_squared = r2_score(test_target, svr_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 490.66103295659246\n",
      "R2 : 0.32516071687253356\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(C=1, epsilon=10, gamma='auto', kernel='linear')\n",
    "svr.fit(train_data, train_target)\n",
    "svr_pred = svr.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, svr_pred)\n",
    "r_squared = r2_score(test_target, svr_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized Scores:\n",
      "MSE: 488.2415548783691\n",
      "R2 : 0.32848838860960294\n",
      "\n",
      "Optimized Scores:\n",
      "MSE: 442.13293825295574\n",
      "R2 : 0.39190468560387814\n"
     ]
    }
   ],
   "source": [
    "elastic = ElasticNet()\n",
    "elastic.fit(train_data, train_target)\n",
    "elastic_pred = elastic.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, elastic_pred)\n",
    "r_squared = r2_score(test_target, elastic_pred)\n",
    "\n",
    "print(\"Unoptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)\n",
    "\n",
    "elastic = ElasticNet(alpha=1e-05, l1_ratio=0.0)\n",
    "elastic.fit(train_data, train_target)\n",
    "elastic_pred = elastic.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, elastic_pred)\n",
    "r_squared = r2_score(test_target, elastic_pred)\n",
    "\n",
    "print(\"\\nOptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized Scores:\n",
      "MSE: 241.94759012509175\n",
      "R2 : 0.6672331257068063\n",
      "\n",
      "Optimized Scores:\n",
      "MSE: 323.58942186541566\n",
      "R2 : 0.5549455962226226\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(train_data, train_target)\n",
    "dt_pred = dt.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, dt_pred)\n",
    "r_squared = r2_score(test_target, dt_pred)\n",
    "\n",
    "print(\"Unoptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)\n",
    "\n",
    "dt = DecisionTreeRegressor(criterion='poisson', max_depth=5, max_features=0.5)\n",
    "dt.fit(train_data, train_target)\n",
    "dt_pred = dt.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, dt_pred)\n",
    "r_squared = r2_score(test_target, dt_pred)\n",
    "\n",
    "print(\"\\nOptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized Scores:\n",
      "MSE: 143.39198810743505\n",
      "R2 : 0.8027833066800633\n",
      "\n",
      "Optimized Scores:\n",
      "MSE: 152.654530627431\n",
      "R2 : 0.7900439058834144\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(train_data, train_target)\n",
    "rfr_pred = rfr.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, rfr_pred)\n",
    "r_squared = r2_score(test_target, rfr_pred)\n",
    "\n",
    "print(\"Unoptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)\n",
    "\n",
    "rfr = RandomForestRegressor(max_depth=80, max_features=3, min_samples_leaf=3, min_samples_split=8, n_estimators=500, n_jobs=-1)\n",
    "rfr.fit(train_data, train_target)\n",
    "rfr_pred = rfr.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, rfr_pred)\n",
    "r_squared = r2_score(test_target, rfr_pred)\n",
    "\n",
    "print(\"\\nOptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized Scores:\n",
      "MSE: 257.2219705167492\n",
      "R2 : 0.6462252379362805\n",
      "\n",
      "Optimized Scores:\n",
      "MSE: 224.46649138793268\n",
      "R2 : 0.6912760623732452\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knn.fit(train_data, train_target)\n",
    "knn_pred = knn.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, knn_pred)\n",
    "r_squared = r2_score(test_target, knn_pred)\n",
    "\n",
    "print(\"Unoptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)\n",
    "\n",
    "knn = KNeighborsRegressor(metric='manhattan', n_jobs=-1, n_neighbors=8)\n",
    "knn.fit(train_data, train_target)\n",
    "knn_pred = knn.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, knn_pred)\n",
    "r_squared = r2_score(test_target, knn_pred)\n",
    "\n",
    "print(\"\\nOptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized Scores:\n",
      "MSE: 132.8607595327332\n",
      "R2 : 0.8172676171601091\n",
      "\n",
      "Optimized Scores:\n",
      "MSE: 727.091786219976\n",
      "R2 : -1.847879361971394e-05\n"
     ]
    }
   ],
   "source": [
    "trees = ExtraTreesRegressor()\n",
    "trees.fit(train_data, train_target)\n",
    "trees_pred = trees.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, trees_pred)\n",
    "r_squared = r2_score(test_target, trees_pred)\n",
    "\n",
    "print(\"Unoptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)\n",
    "\n",
    "trees = ExtraTreesRegressor(min_samples_leaf=1.0, min_samples_split=0.1, n_estimators=250, n_jobs=-1)\n",
    "trees.fit(train_data, train_target)\n",
    "trees_pred = trees.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, trees_pred)\n",
    "r_squared = r2_score(test_target, trees_pred)\n",
    "\n",
    "print(\"\\nOptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized Scores:\n",
      "MSE: 1.5627072578980882e+28\n",
      "R2 : -2.1492969174736365e+25\n",
      "\n",
      "Optimized Scores:\n",
      "MSE: 892.9276043163728\n",
      "R2 : -0.22810368850891805\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDRegressor()\n",
    "sgd.fit(train_data, train_target)\n",
    "sgd_pred = sgd.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, sgd_pred)\n",
    "r_squared = r2_score(test_target, sgd_pred)\n",
    "\n",
    "print(\"Unoptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)\n",
    "\n",
    "sgd = SGDRegressor(alpha=1000.0, loss='epsilon_insensitive', max_iter=1500, penalty='l1')\n",
    "sgd.fit(train_data, train_target)\n",
    "sgd_pred = sgd.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, sgd_pred)\n",
    "r_squared = r2_score(test_target, sgd_pred)\n",
    "\n",
    "print(\"\\nOptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized Scores:\n",
      "MSE: 441.9254330396055\n",
      "R2 : 0.39219008154938184\n",
      "\n",
      "Optimized Scores:\n",
      "MSE: 441.9254330462792\n",
      "R2 : 0.3921900815402031\n"
     ]
    }
   ],
   "source": [
    "bayes = BayesianRidge()\n",
    "bayes.fit(train_data, train_target)\n",
    "bayes_pred = bayes.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, bayes_pred)\n",
    "r_squared = r2_score(test_target, bayes_pred)\n",
    "\n",
    "print(\"Unoptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)\n",
    "\n",
    "bayes = BayesianRidge(alpha_init=1.2, lambda_init=0.0001)\n",
    "bayes.fit(train_data, train_target)\n",
    "bayes_pred = bayes.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(test_target, bayes_pred)\n",
    "r_squared = r2_score(test_target, bayes_pred)\n",
    "\n",
    "print(\"\\nOptimized Scores:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 :\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_data(replace_inf=True)\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(SVR())\n",
    "\tmodels.append(SVR())\n",
    "\tmodels.append(ElasticNet(alpha=1e-05, l1_ratio=0.0))\n",
    "\tmodels.append(DecisionTreeRegressor())\n",
    "\tmodels.append(RandomForestRegressor())\n",
    "\tmodels.append(KNeighborsRegressor(metric='manhattan', n_jobs=-1, n_neighbors=8))\n",
    "\t# models.append(ExtraTreesRegressor())\n",
    "\t# models.append(SGDRegressor(alpha=1000.0, loss='epsilon_insensitive', max_iter=1500, penalty='l1'))\n",
    "\t# models.append(BayesianRidge(alpha_init=1.2, lambda_init=0.0001))\n",
    "\n",
    "\treturn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_super_learner(X):\n",
    "\tensemble = SuperLearner(scorer=r2_score, folds=10, shuffle=True, sample_size=len(X))\n",
    "\t# add base models\n",
    "\tmodels = get_models()\n",
    "\tensemble.add(models)\n",
    "\t# add the meta model\n",
    "\tensemble.add_meta(LinearRegression())\n",
    "\n",
    "\treturn ensemble\n",
    "ensemble = get_super_learner(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  score-m  score-s   ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  decisiontreeregressor       0.65     0.03   1.56  0.50  0.00  0.00\n",
      "layer-1  elasticnet                  0.39     0.03   3.59  0.59  0.00  0.00\n",
      "layer-1  kneighborsregressor         0.69     0.02   0.00  0.00  0.76  0.17\n",
      "layer-1  randomforestregressor       0.81     0.02  52.52  0.65  0.08  0.02\n",
      "layer-1  svr-1                       0.08     0.01  40.18  8.11  4.24  0.72\n",
      "layer-1  svr-2                       0.08     0.01  33.90  6.06  2.33  1.08\n",
      "\n",
      "\n",
      "Training R2 0.9744798838687548 \n",
      "CV R2 0.8045297585240022\n",
      "\n",
      "Training MSE 18.586935131064834 \n",
      "CV MSE 142.12218078118585\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore') #got tired of non-converging erros\n",
    "\n",
    "#fit the super learner\n",
    "ensemble.fit(train_data.values,train_target.values)\n",
    "#summarize base learners\n",
    "print(ensemble.data)\n",
    "\n",
    "train_pred = ensemble.predict(train_data)\n",
    "test_pred = ensemble.predict(test_data)\n",
    "\n",
    "#obtain scores for the model\n",
    "training_r2 = r2_score(train_target,train_pred)\n",
    "test_r2 = r2_score(test_target,test_pred)\n",
    "\n",
    "training_mse = mean_squared_error(train_target,train_pred)\n",
    "test_mse = mean_squared_error(test_target,test_pred)\n",
    "\n",
    "print(\"\\nTraining R2\",training_r2,\"\\nCV R2\",test_r2)\n",
    "print(\"\\nTraining MSE\",training_mse,\"\\nCV MSE\",test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_one(\"Superlearner\", get_super_learner, {'X': train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('../data/supercon_ml_latest_run.db') #this can dump a python session so I can resume later, after restarts and such"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CLASSE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cd1206b64939dd3f6b0a0731e141469e9fe64d6063519119675fbaf8ff2b829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
