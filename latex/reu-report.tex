%% ****** Start of file apsguide4-2.tex ****** %
%%
%%   This file is part of the APS files in the REVTeX 4.2 distribution.
%%   Version 4.2b of REVTeX, December 2018.
%%
%%   Copyright (c) 2019 The American Physical Society.
%%
%%   See the REVTeX 4.2 README file for restrictions and more information.
%%
\documentclass[twocolumn, nofootinbib, secnumarabic, amssymb, nobibnotes, aps, prd]{revtex4-2}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}

\newcommand{\revtex}{REV\TeX\ }
\newcommand{\classoption}[1]{\texttt{#1}}
\newcommand{\macro}[1]{\texttt{\textbackslash#1}}
\newcommand{\m}[1]{\macro{#1}}
\newcommand{\env}[1]{\texttt{#1}}
\setlength{\textheight}{9.5in}

\begin{document}

%show just eight optimized models w/o error
%comparison of base model, then cleaned - extra trees
%

\begin{abstract} % abstract, send to suchi to revise
Abstract will be added here.
\end{abstract}


\title{Predicting Superconducting Critical Temperatures with Uncertainty Quantification using Supervised Machine Learning}

\author{K. Kleinasser, Cornell University, Ithaca, NY}
\altaffiliation{Lycoming College, Williamsport, PA}
\email{klekirk@lycoming.edu}

\maketitle
%\tableofcontents

\section{Introduction}
\subsection{Superconductors} %get rid of subsections, bulletpoint types of models, incl. references
%model discussions after dataset
Superconductors are materials that lose all electrical resistance at low temperatures. These materials have a critical temperature ($T_C$) at which they lose their resistance. Most have very low critical temperatures, but “unconventional superconductors” can have critical temperatures as high as room temperature under non-atmospheric conditions. 

Electrons in superconductors form Cooper Pairs below their critical temperature. These pairs of electrons are held together with phonouns, which are atomic-level collective excitations. Phonouns are similar to photons in that they also have particle-like properties \cite{rohlf_1994}.

Unconventional superconductors are still not well understood and remain an open question in Physics. Understanding them could lead to the discovery of superconducting materials stable at room temperature under atmospheric conditions. Such a material would have large implications, such as super efficient electricity transfer and vast efficiency improvements for applications like particle accelerators and power lines.


\subsection{Matminer}
Most superconducter databases do not include enough information to train an effective machine learning model, but such data can be extracted from the data they do provide. We use matminer to produce our features from the provided material data. Matminer is a python library that generates data from various measured properties of a material \cite{WARD201860}. Matminer collects existing calculations into a machine learning friendly python package. %Our matminer workflow is shown in Figure~\ref{fig:matminer-flowchart}.

% \begin{figure}[!htb]
%    \centering
%    \includegraphics[scale=0.25]{flowchart.png}
%    \caption{Flowchart illustrating our matminer usage, modified from official matminer graphic \cite{WARD201860}.}
%    \label{fig:matminer-flowchart}
% \end{figure}

Our database only provides the superconductor composition data. Matminer's featurizers can generate 53 features from the composition of a material. If we had band structure or other data, we could produce more information that we could use in our model.

\subsection{Machine Learning}
Previous papers have used random forest models to predict critical temperature [citation needed], but this paper will examine eight models before settling on two for further investigation. All models are implemented with Scikit-Learn, with the notable exception of a mlens superlearner \cite{scikit-learn, flennerhag:2017mlens}. We will also use MAPIE models for uncertainty, discussed in Section~\ref{sec:uncertainty}.

We started our model search with some linear models. Besides the base Linear Regression model, we used linear (and polynomial) Support Vector Regression (SVR) models. SVR uses decision boundaries, which are lines parallel to the regression line. The model aims to maximize the amount of data within the decision boundaries and has hyperparamters to modify sensitivity to prevent overfitting.\footnote{Overfitting occurs when a model is trained to be too specific to a particular dataset and is not generalizable.} We also trialed Elastic Net and Bayesian Ridge models. Elastic Net uses L1 and L2 penalties to stabilize the model, and Bayesian Ridge uses probability distributors instead of point estimates.

Additionally, we trialed Decision Tree and KNeighbors (KNN) models. Decision trees are very interpretable - they break predictions into nodes of the tree, eventually leading to a prediction value. These trees can be represented graphically and show how they produce results, unlike most machine learning models. KNN models are a little different, they store all the data and predict values based on a similarity measure. The model looks at a specified number of similar neighbors to produce a prediction.

Finally, we tried multiple ensemble models - Random Forest Regression (RFR), Extra Trees, and a superlearner. RFR models use numerous decision trees and subsamples the data with replacement. This means that the model replace data after using it in a subset. Extra Trees is like RFR, but it does not replace the data after use in a subset. The final ensemble model we tested is a superlearner, a model that can combine multiple high-scoring Scikit-Learn model predictions and sometimes improve the performance from the individual models.


\section{Methodology}
\subsection{Datasets}
We chose to use one of the most popular experimental datasets, the supercon database from Japan's National Institute for Materials Science. This datasets contains 16,414 superconductor chemical compositions and their experimentally measured critical temperatures. Unfortunately, the database is not currently available on their website for unspecified reasons, so we obtained the dataset from a github repository that used this data \cite{vstanev1_2018}. 

In this dataset, there are 10,154 samples with a critical temperature below 10K and 6,210 samples above 10K. There are 159 samples with temperatures above 100K and 0 samples above 260K. A histogram represents this information graphically in Figure~\ref{fig:data-histogram}.

 \begin{figure}[!htb]
    \centering % add numbers to columns
    \includegraphics[width=\columnwidth]{dataset_histogram.png}
    \caption{Critical temperature histogram of the dataset.} %more info in caption
    \label{fig:data-histogram}
 \end{figure}


\subsection{Code Structure} %MENTION PANDAS/NUMPY/ETC
The source code used for this paper is available publicly on github at \url{https://github.com/sylphrena0/classe2022}. This repository also includes the source files for this latex paper, data files, images, and documentation files.

Our research uses numpy and pandas throughout our code to handle arrays and tabular data \cite{Harris2020array, Reback2020pandas}. We also use matplotlib and seaborns to generate our graphs \cite{Hunter2007, Waskom2021}.

The code is split into multiple python files so processes could be completed in stages and to maintain readability in the code. Most of our testing and final training was completed in juypter notebooks, but some computations were highly computationally expensive and needed to be run remotely. For these jobs, we created simple python files and made bash scripts to run them on Cornell's CLASSE compute farm. We also made several bash aliases and functions to simplify the compute farm workflow, which are also available on the github repository. 

Since we used multiple files, we chose to create shared dependencies files where we defined functions to import data, train models, and generate our graphs. These files are then imported in all the relevant scripts to reduce redundancy. More detailed explainations of the purpose of each file is available in the github readme file and documentation within the files.

\subsection{Code Evaluation}

First, the featurizer script imports the dataset, extracts features from the material compositions, and exports the csv data. This script is one of the most computationally expensive and takes several hours to run on the CLASSE compute farm with 64 dedicated cores.

After the features are exported, our analysis jupyter notebook imports the data with the shared import function and exports histograms and a correlation matrix. 

Next, the training\_single jupyter notebook or script can train individual models with the shared evaluation functions. This is used to get a landscape of initial performance before optimization. After training, the function plots the actual $T_C$ versus the model prediction, using a heatmap to visualize the difference from the ideal prediction.

The optimizer script then uses a grid of manually defined hyperparameters to optimize models based on R2 score. This allowed significant improvements to baseline models. After optimization, the optimized models can be plotted in our single training notebook. After confirmation that the model is better than the baseline, the models can then be plotted together in a single graph using our bulk training notebook. 

We evaluated our models using several metrics - R2 scores (R2) for regression evaluation, Mean Squared Error (MSE) and Mean Absolute Error (MAE) for error evaluation, and MAPIE Effective Mean Widths (MWS) for uncertainty evaluation.

\subsection{Uncertainty}\label{sec:uncertainty}
Our evaluation functions can produce uncertainty calculations using forestci, mapie, or lolopy \cite{Polimis2017, Taquet2022, Hutchinson2022}. 

Forestci is python implementation of an algorithm from \cite{Wager2014} that predicts confidence intervals for random forest models. It is the fastest of the uncertainty methods listed. 

The Model Agnostic Prediction Interval Estimator (MAPIE) python library is more recent implementation of jackknife based on \cite{Foygel2020}. MAPIE uses various resampling methods. Most methods require the use of MAPIE's own MapieRegressor, which accepts an Scikit-Learn regressor and keeps track of uncertainty as the model is trained. MAPIE also has a prefit method, but it is difficult to extract uncertainty bars for individual points from this data - it splits a celebration set off the test set to generate uncertainty, so it can't be easily added to our plot of test set predictions. Thus, we will only compare the normal MAPIE methods with the other libraries. MAPIE trains much slower than other models, particularly on our superlearner, but it is still considerably faster than our final uncertainty model, lolopy.

Lolo is a Scala random forest machine learning library and is not a native python implementation. Lolopy is a python wrapper for lolo, but this implementation is very slow for large datasets. 

\section{Results}
\subsection{Model Optimization}
Each model's hyperparameters\footnote{Hyperparameters are machine learning parameters that change how a model is trained.} were optimized with various optimization methods. Optimization can be computationally expensive, so we chose to optimize on a randomly selected subset of 2,000 materials, using a numpy random state for reproducibility. To start, we used Scikit-Learn's GridSearchCV, which tests combinations from a grid of hyperparameters and returns the best performing model based on a specified metric. 

We also implemented Bayesian optimization using Gaussian Processes methods from the Scikit-Optimize library \cite{head_tim_2021_5565057}. Bayesian optimization attempts to optimize models intelligently, instead of randomly testing specified hyperparameters. We provide a range of hyperparamters values for Bayesian optimization to test, and the algorithm uses acquisition functions to decide which specific values to use within the specified range. This is different from GridSearchCV, which is simpler but can take much longer to find optimial hyperparameters. We only implemented Bayesian optimization on our top models.

The performance of each optimizer on selected high performance models is shown in Table~\ref{tab:optimizers}. Note that the optimal method for each model (shown in bold), was found with a different method each time. The Bayesian optimization was much less computationally expensive than GridSearchCV, however, and is the recommended method for large datasets.

\begin{table}[!htb] %remove red and orange
\centering 
\resizebox{\columnwidth}{!}{%
\begin{tabular}{crcccc}
\hline
\multicolumn{1}{l}{\textbf{Model}}           & \multicolumn{1}{r}{\textbf{Optimizer}} & \textbf{R2}                   & \textbf{MSE}                    & \textbf{MAE}                  & \textbf{MWS}                   \\ \hline
                               & Base Model                             & \cellcolor[HTML]{F4CCCC}0.807 & \cellcolor[HTML]{F4CCCC}140.482 & \cellcolor[HTML]{F4CCCC}5.791 & \cellcolor[HTML]{F4CCCC}53.135 \\
                               & GridSearchCv                           & \cellcolor[HTML]{F4CCCC}0.804 & \cellcolor[HTML]{F4CCCC}142.524 & \cellcolor[HTML]{F4CCCC}5.816 & \cellcolor[HTML]{F4CCCC}52.416 \\
                               & \textbf{Bayesian – PI}                 & \cellcolor[HTML]{D9EAD3}0.815 & \cellcolor[HTML]{D9EAD3}134.509 & \cellcolor[HTML]{D9EAD3}5.72  & \cellcolor[HTML]{D9EAD3}50.985 \\
                               & Bayesian – EI                          & \cellcolor[HTML]{FCE5CD}0.814 & \cellcolor[HTML]{FCE5CD}135.267 & \cellcolor[HTML]{FCE5CD}5.763 & \cellcolor[HTML]{FCE5CD}51.533 \\
\multirow{-5}{*}{\textbf{\rotatebox[origin=c]{90}{\parbox[c]{2cm}{\centering Random Forest}}}} & Bayesian – gp\_hedge                   & \cellcolor[HTML]{D9EAD3}0.815 & \cellcolor[HTML]{D9EAD3}134.785 & \cellcolor[HTML]{D9EAD3}5.747 & \cellcolor[HTML]{D9EAD3}51.021 \\ \hline
                               & Base Model                             & \cellcolor[HTML]{FCE5CD}0.818 & \cellcolor[HTML]{FCE5CD}132.395 & \cellcolor[HTML]{FCE5CD}5.214 & \cellcolor[HTML]{FCE5CD}48.153 \\
                               & GridSearchCv                           & \cellcolor[HTML]{FCE5CD}0.818 & \cellcolor[HTML]{FCE5CD}132.374 & \cellcolor[HTML]{FCE5CD}5.215 & \cellcolor[HTML]{FCE5CD}48.231 \\
                               & Bayesian – PI                          & \cellcolor[HTML]{FCE5CD}0.818 & \cellcolor[HTML]{FCE5CD}132.104 & \cellcolor[HTML]{FCE5CD}5.225 & \cellcolor[HTML]{FCE5CD}48.154 \\
                               & Bayesian – EI                          & \cellcolor[HTML]{F4CCCC}0.816 & \cellcolor[HTML]{F4CCCC}133.5   & \cellcolor[HTML]{F4CCCC}5.257 & \cellcolor[HTML]{F4CCCC}48.757 \\
\multirow{-5}{*}{\textbf{\rotatebox[origin=c]{90}{\parbox[c]{1cm}{\centering Extra Trees}}}}  & \textbf{Bayesian – gp\_hedge}          & \cellcolor[HTML]{D9EAD3}0.819 & \cellcolor[HTML]{D9EAD3}131.783 & \cellcolor[HTML]{D9EAD3}5.202 & \cellcolor[HTML]{D9EAD3}47.732 \\ \hline
                               & Base Model                             & \cellcolor[HTML]{FCE5CD}0.646 & \cellcolor[HTML]{FCE5CD}257.108 & \cellcolor[HTML]{F4CCCC}8.563 & \cellcolor[HTML]{FCE5CD}75.612 \\
                               & \textbf{GridSearchCv}                  & \cellcolor[HTML]{D9EAD3}0.703 & \cellcolor[HTML]{D9EAD3}216.186 & \cellcolor[HTML]{D9EAD3}7.515 & \cellcolor[HTML]{D9EAD3}69.758 \\
                               & Bayesian – PI                          & \cellcolor[HTML]{D9EAD3}0.652 & \cellcolor[HTML]{FCE5CD}253.137 & \cellcolor[HTML]{FCE5CD}8.201 & \cellcolor[HTML]{D9EAD3}74.177 \\
                               & Bayesian – EI                          & \cellcolor[HTML]{D9EAD3}0.652 & \cellcolor[HTML]{FCE5CD}253.137 & \cellcolor[HTML]{FCE5CD}8.201 & \cellcolor[HTML]{D9EAD3}74.177 \\
\multirow{-5}{*}{\textbf{\rotatebox[origin=c]{90}{\parbox[c]{2cm}{\centering KNN}}}} & Bayesian – gp\_hedge                   & \cellcolor[HTML]{F4CCCC}0.566 & \cellcolor[HTML]{F4CCCC}315.238 & \cellcolor[HTML]{FCE5CD}8.159 & \cellcolor[HTML]{F4CCCC}84.546 \\ \hline
\end{tabular}%
}
\caption{Comparison of optimization methods by model.}
\label{tab:optimizers}
\end{table}

 \onecolumngrid
 \begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.35]{results_optimized.png}
    \caption{Graph of optimized model predictions versus actual critical temperatures for each base model.}
    \label{fig:results}
 \end{figure}
 \twocolumngrid

\subsection{Base Models}
Our base models provided good results and the top two models, Random Forest and Extra Trees, only improved their scores marginally with optimization. Our numerical results of the optimized and optimized models are shown in Table~\ref{tab:results}.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[!hbt]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{clcccc}
\hline
\multicolumn{1}{l}{}                                                                                 & \textbf{Model}                    & \multicolumn{1}{l}{\textbf{R2}} & \multicolumn{1}{l}{\textbf{MSE}} & \multicolumn{1}{l}{\textbf{MAE}} & \multicolumn{1}{l}{\textbf{MWS}} \\ \hline
\multirow{8}{*}{\textbf{\textbf{\rotatebox[origin=c]{90}{\parbox[c]{2cm}{\centering Unoptimized}}}}} & \textbf{Extra Trees Regression}   & 0.818                           & 132.395                          & 5.214                            & 48.153                           \\
                                                                                                     & \textbf{Random Forest Regression} & 0.807                           & 140.482                          & 5.791                            & 53.135                           \\
                                                                                                     & \textbf{KNeighbors Regression}    & 0.646                           & 257.108                          & 8.563                            & 75.612                           \\
                                                                                                     & \textbf{Decision Tree Regression} & 0.644                           & 258.945                          & 6.886                            & 76.715                           \\
                                                                                                     & \textbf{Bayesian Regression}      & 0.392                           & 441.925                          & 14.52                            & 90.7                             \\
                                                                                                     & \textbf{Linear Regression}        & 0.392                           & 442.163                          & 14.506                           & 90.705                           \\
                                                                                                     & \textbf{Elastic Net Regression}   & 0.328                           & 488.242                          & 15.603                           & 97.938                           \\
                                                                                                     & \textbf{Support Vector Machines}  & 0.084                           & 666.16                           & 15.511                           & 134.628                          \\ \hline
\multirow{8}{*}{\textbf{\textbf{\rotatebox[origin=c]{90}{\parbox[c]{2cm}{\centering Optimized}}}}}   & \textbf{Extra Trees Regression}   & 0.819                           & 131.624                          & 5.205                            & 48.088                           \\
                                                                                                     & \textbf{Random Forest Regression} & 0.816                           & 133.87                           & 5.714                            & 50.759                           \\
                                                                                                     & \textbf{KNeighbors Regression}    & 0.703                           & 216.186                          & 7.515                            & 69.758                           \\
                                                                                                     & \textbf{Decision Tree Regression} & 0.664                           & 244.095                          & 7.152                            & 74.834                           \\
                                                                                                     & \textbf{Elastic Net Regression}   & 0.392                           & 442.133                          & 14.487                           & 90.713                           \\
                                                                                                     & \textbf{Bayesian Regression}      & 0.392                           & 441.925                          & 14.52                            & 90.7                             \\
                                                                                                     & \textbf{Linear Regression}        & 0.392                           & 442.163                          & 14.506                           & 90.705                           \\
                                                                                                     & \textbf{Support Vector Machines}  & 0.325                           & 490.661                          & 14.186                           & 106.946                          \\ \hline
\end{tabular}%
}
\caption{CV scores of the models using all features and data, comparing optimized and base hyperparameters.}
\label{tab:results}
\end{table}

\subsection{Cleaned Models}
Our intial

\subsection{Error Comparison}
Our intial

\subsection{Feature Importance}
Our intial

\section{Conclusion}
\subsection{Future Work}

\begin{acknowledgments}
I would like to thank my mentor, Suchismita Sarker, for her guidance and support with this project. This work is supported by the U.S. National Science Foundation under award number NSF PHY-2150125, REU Site: Accelerator Physics and Synchrotron Radiation Science.
\end{acknowledgments}

\bibliographystyle{apsrev4-2}
\bibliography{reu-report}



\end{document}